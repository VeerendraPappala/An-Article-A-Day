# An-Article-A-Day

## Day-1 | Build Book Recommender Systems. 


![book-shelf](https://user-images.githubusercontent.com/45025357/52583901-b168a080-2e56-11e9-8ab2-511ace369f80.jpg)

  **Article Is about Building Recommendation System for Books using Booksdata Set.** 
  
* Virtually everyone has had an online experience where a website makes personalized recommendations. Amazon tells you ‚ÄúCustomers Who Bought This Item Also Bought‚Äù, Udemy tells you ‚ÄúStudents Who Viewed This Course Also Viewed‚Äù. Building recommender systems today requires specialized expertise in analytics, machine learning and software engineering, and learning new skills and tools is difficult and time-consuming.  
* Checkout the code from here -  [DAY1](https://github.com/VeerendraPappala/BOOK-RECOMMENDATION-SYSTEM/blob/master/Books_Recommendation.ipynb)


## Day-2 | Summarize Trump‚Äôs State of the Union Address

![trump](https://user-images.githubusercontent.com/45025357/52652831-d3c0f380-2f14-11e9-887b-a564feaee770.jpg)

**Article Is about build a Text summarizer so as to summarize Trumps reactions on State of White Houses instead of listening entire speech for  82minutes** 

* Automatic text summarization, is the process of creating a short, concise and coherent version of a longer document
 
* Checkout the code from here - [Day2](https://github.com/VeerendraPappala/Analyise-Summarize-Trump-s-remarks-on-state-of-the-Union-Houses/blob/master/Summarize-Trump's-Remarks.ipynb)

## Day-3 | Time Forecast using  Tpot Automated ML in python

![tpot-logo](https://user-images.githubusercontent.com/45025357/52727507-5f9d5300-2fdb-11e9-99b7-d86acc80f44a.jpg)

**Article is about Time Forecast Using TPOT- An automated Machine Learning In Python**

* TPOT is meant to be an assistant that gives you ideas on how to solve a particular machine learning problem by exploring pipeline configurations that you might have never considered, then leaves the fine-tuning to more constrained parameter tuning techniques such as grid search.

* So TPOT helps you find good algorithms. Note that it isn‚Äôt designed for automating deep learning‚Ää‚Äî‚Ääsomething like AutoKeras might be helpful there.
* AutoML algorithms aren‚Äôt as simple as fitting one model on the dataset; they are considering multiple machine learning algorithms (random forests, linear models, SVMs, etc.) in a pipeline with multiple preprocessing steps (missing value imputation, scaling, PCA, feature selection, etc.), the hyperparameters for all of the models and preprocessing steps, as well as multiple ways to ensemble or stack the algorithms within the pipeline.

* There are so many interesting directions to explore with TPOT and autoML. I‚Äôd like to compare TPOT with autoSKlearn, MLBox, Auto-Keras, and others. I‚Äôd also like to see how it performs with a greater variety of data, other imputation strategies, and other encoding strategies. A comparison with LightGBM, CatBoost , and deep learning algorithms would also be interesting. 

* For More info info Read the complete [Article](https://towardsdatascience.com/time-forecast-with-tpot-b2d87eaba59c) & [Documentation](http://epistasislab.github.io/tpot/using/)

* Checkout the complete Code here - [Day3](https://github.com/VeerendraPappala/Time-Forecast-Using-TPOT/blob/master/TPOT.ipynb)

## Day-4 | PYTORCH AND ResNet for Traffic Sign Classification With PyTorch

![pytorch-logo](https://user-images.githubusercontent.com/45025357/52785901-36360300-307f-11e9-97f4-d01c619a51ab.jpg)

**Article is Restnet used  with pytorch to classify German Traffic Board signs using Fastai in Python** 

* PyTorch was one of the most popular frameworks in 2018.PyTorch is a Python based scientific computing package that is similar to NumPy, but with the added power of GPUs. It is also a deep learning framework that provides maximum flexibility and speed during implementing and building deep neural network architectures.

* The FastAi library is a high level library build on PyTorch, which allows us to build models using only a few lines of code.The FastAI library provides a lot of different datasets which can be loaded in directly, but it also provides functionality for downloading images given a file containing the urls of these images.

* Fastai  allows us to perform transfer learning with less code and time by giving us the ability to set different learning rates for different parts in the network. This allows us to train the earlier layers less than the latter layers.

* Get here  more Info on [pytorch official Guide](https://pytorch.org/tutorials/), 
[Deep Learning with PyTorch](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)
,[ Introduction on pytorch](https://www.analyticsvidhya.com/blog/2019/01/guide-pytorch-neural-networks-case-studies/),  [Resnet for traffic signs Classification using Pytorch](https://towardsdatascience.com/resnet-for-traffic-sign-classification-with-pytorch-5883a97bbaa3)

* Code for - [Day4](https://github.com/VeerendraPappala/PYTORCH/blob/master/Resnet.ipynb)


## Day-5 | Google Facets & Bookeh For Data Visualisation in Python

![data-visualization-tools-in-python-15-638](https://user-images.githubusercontent.com/45025357/52860670-1b828d80-3156-11e9-98f1-b0b950b93ae8.jpg)

**Visualising Machine Learning Datasets with Google‚Äôs FACETS - An open source tool from Google to easily learn patterns from large amounts of data And Bokeh**

* Data visualization is important to create meaningful information and decision making power. Data is useless if its not creating any meaning. I did  EDA with FACETS on the universal Dataset and Heart Disease dataset from Kaggle. 
* more info is here 
  * [Article on Facets](https://towardsdatascience.com/visualising-machine-learning-datasets-with-googles-facets-462d923251b3)            
  * [Article on Bokeh](https://towardsdatascience.com/interactive-plotting-with-bokeh-ea40ab10870)
  * [Documentation on Bokeh](https://bokeh.pydata.org/en/latest/docs/user_guide/layout.html)
  * [Facets](https://ai.googleblog.com/2017/07/facets-open-source-visualization-tool.html)
  
* Notebooks of Day-5 - [Facets](https://github.com/VeerendraPappala/Data-Visualization/tree/master/FACETS) & [Bokeh](https://github.com/bokeh/bokeh-notebooks/tree/master/tutorial)



## Day-6 | Develop a NLP Model in Python & Deploy It with Flask


![flask](https://user-images.githubusercontent.com/45025357/52902710-9a50f680-323a-11e9-92ea-49942e0b974b.png)

**In reality, generating predictions is only part of a machine learning project, although it is the most important part . This article is about how to deploying a model in Production using Flask.**

* For more info, chekout these articles
  * [Tutorial to deploy Machine Learning models in Production as APIs](https://www.analyticsvidhya.com/blog/2017/09/machine-learning-models-as-apis-using-flask/)
  * [Deploying a Machine Learning Model as a REST API](https://towardsdatascience.com/deploying-a-machine-learning-model-as-a-rest-api-4a03b865c166)
  * [NLP model Deployement Using Flask](https://towardsdatascience.com/develop-a-nlp-model-in-python-deploy-it-with-flask-step-by-step-744f3bdd7776)
  
* The complete working source code is available at this repository - [Day-6](https://github.com/susanli2016/SMS-Message-Spam-Detector) 


## Day-7 | PyViz: Simplifying the Data Visualisation process in Python.

![pyviz](https://user-images.githubusercontent.com/45025357/52916608-b8d0f380-3307-11e9-8b64-9e6448802ea3.png)

**Article today is about An overview of the PyViz ecosystem to make data visualizations in Python easier to use, learn and more powerful.**

* Now, to choose the best tool for our job from amongst all of the above  is a bit tricky and confusing. PyViz tries to plug this situation. It helps to streamline the process of working with small and large datasets (from a few points to billions) in a web browser, whether doing exploratory analysis, making simple widget-based tools or building full-featured dashboards.
* HoloViews : Declarative objects for instantly visualizable data, building Bokeh plots from convenient high-level specifications.
* The open source libraries which constitute PyViz are:
  * GeoViews 
  * Datashader
  * hvPlot 
  * Param 
  * Holoviews
  * Bokeh
  * Panel

* Get more info by reading these [Article by Parul Pandey](https://towardsdatascience.com/pyviz-simplifying-the-data-visualisation-process-in-python-1b6d2cb728f1), [Pyviz Documentation](http://pyviz.org).
* The dataset used in practise is [Uniqlo (FastRetailing) Stock Price Prediction](https://www.kaggle.com/daiearth22/uniqlo-fastretailing-stock-price-prediction) from Kaggle.

* Here is the Data and Notebook of  [Day-7](https://github.com/VeerendraPappala/Data-Visualization/tree/master/Pyviz)

## DAY-8 |  A Linear Regression with PySpark and MLlib

![pyspark](https://user-images.githubusercontent.com/45025357/52965852-cb146580-33cb-11e9-91ba-af4f54fad2e3.png)

**Apache Spark has become one of the most commonly used and supported open-source tools for machine learning and data science.**
*  From this [Article](https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a),I got to know how to  get started using Apache Spark‚Äôs.
* Apache Spark‚Äôs.spark.ml Linear Regression is used for predicting Boston housing prices. 
*  data is from the [Kaggle competition](https://www.kaggle.com/c/boston-housing/data)
* For more info on pyspark dive into these links [Apache Spark](https://spark.apache.org), [Apache spark for classification & Regression](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#linear-regression)
* Notebook of  [Day-8](https://github.com/VeerendraPappala/PYSPARK)

## DAY-9 | A Logistic Regression with Pyspark

![merge](https://user-images.githubusercontent.com/45025357/53030592-23fa0180-3491-11e9-9fad-5e1a1daf5bfd.jpg)

* From this [Article](https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a),I got to know how to  get started using Apache Spark‚Äôs. 
* As the article tells about linear regression, I built a logistic regression with Apache Spark using [banks marketing Dataset](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) taken from UCI. 
* For more info on [Apache Spark](https://spark.apache.org), [Apache spark for classification & Regression](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#linear-regression).
 
* Notebook of [Day-9](https://github.com/VeerendraPappala/PYSPARK/tree/master/Logistic%20Regression)

## DAY-10 | K-Means with Pyspark

![k-means_convergence](https://user-images.githubusercontent.com/45025357/53107879-05ad0800-355c-11e9-884b-8528fe3aed1f.gif)

*  k-means clustering aims to converge on an optimal set of cluster centers (centroids) and cluster membership based on distance from these centroids via successive iterations, it is intuitive that the more optimal the positioning of these initial centroids, the fewer iterations of the k-means clustering algorithms will be required for convergence. 
* Today I've learned  kmeans implementation using pyspark in python. 
* Article on K-means uisng pyspark is [here](https://towardsdatascience.com/k-means-implementation-in-python-and-spark-856e7eb5fe9b) 
* Notebook of [Day-9](https://github.com/VeerendraPappala/PYSPARK/tree/master/K-MEANS)

## Day-11 | Classification with Pyspark

![binary](https://user-images.githubusercontent.com/45025357/53183825-0d36e480-3622-11e9-8e20-1dcbd63b7ceb.jpg)


* Binary classification involves classifying the data into two groups, e.g. whether or not a customer buys a particular product or not (Yes/No), based on independent variables such as gender, age, location etc.
* As the target variable is not continuous, binary classification model predicts the probability of a target variable to be Yes/No. 
* I learned implementing  Binary Classification using Pyspark in Python as of today's work
* [Article](https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a) I learned today. 
* Notebook of [Day-10](https://github.com/VeerendraPappala/PYSPARK)

## Day-12 | NLP with Pyspark

![what-is-nlp](https://user-images.githubusercontent.com/45025357/53259837-23ae7000-36f6-11e9-9adf-fee737336343.png)


* A Natural language or Ordinary language is any language that has evolved naturally with time in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forms, such as speech, signing or text.
*  Natural language processing is a field of Artificial Intelligence that explores computational methods for interpreting and processing natural language, in either textual or spoken form.
* To implement NLP we have some useful tools available:
   * CoreNLP 
   * NLTK, the most widely-mentioned NLP library for Python
   * TextBlob
   * Gensi,
   * SpaCy
* NLTK environment setup and Installation in Apache Spark
   * Word tokenize
   * Remove Stopwords
   * Remove punctuations
   * Part of speech tagging
   * Named Entity Recognition
   * Lemmatization
   * Text Classification
   
* Notebook - [Day-12](https://github.com/VeerendraPappala/PYSPARK)

## Day-13 | Decision Tree & Random Forest In Pyspark

![decisiontree](https://user-images.githubusercontent.com/45025357/53289359-3343bc80-37bb-11e9-86d1-f78673687821.jpg)

* Notebook - [Day-13](https://github.com/VeerendraPappala/PYSPARK/tree/master/DecisionTrees%20%26%20RandomForest)

## Day-14 | Bringing the best out of Jupyter Notebooks for Data Science

![jupyter](https://user-images.githubusercontent.com/45025357/53302551-32775d00-3885-11e9-8350-9a87087413d7.png)


* Enhancing Jupyter Notebook‚Äôs productivity with these Tips & Tricks.
* Exploring Jupyter Notebooks‚Äô features which can enhance our productivity while working with them.   

* Notebook - [Day-14](https://github.com/VeerendraPappala/An-Article-A-Day/tree/master/Day-14)

## Day-15 | Data visualtion with  Matplotlib, seaborn

![plots](https://user-images.githubusercontent.com/45025357/53352864-9ebb9480-3949-11e9-945f-12de911fd706.png)


* Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy.
* Seaborn is a library for making statistical graphics in Python. It is built on top of matplotlib and closely integrated with pandas data structures.
* Get in touch with its official documentation here:
      * [Seaborn](https://seaborn.pydata.org)
      * [Matplotlib](https://matplotlib.org)

* Notebook - [Day-15](https://github.com/VeerendraPappala/Data-Visualization/tree/master/Effective%20data%20visualization)

## Day-16 | Introduction to pytorch

![pytorch-logo](https://user-images.githubusercontent.com/45025357/52785901-36360300-307f-11e9-97f4-d01c619a51ab.jpg)

PyTorch is a Python machine learning package based on Torch, which is an open-source machine learning package based on the programming language Lua. PyTorch has two main features:
   * Tensor computation (like NumPy) with strong GPU acceleration
   * Automatic differentiation for building and training neural networks

* Notebook - [Day-16](https://github.com/VeerendraPappala/An-Article-A-Day/tree/master/Day-16)

## Day-17 | Multi-Class Text Classification with Scikit-Learn

![text-scikit](https://user-images.githubusercontent.com/45025357/53511195-d52e1680-3ae5-11e9-9fdc-7ed4c60eefb8.png)

* There are lots of applications of text classification in the commercial world. 
* However, the vast majority of text classification articles and tutorials on the internet are binary text classification such as email spam filtering (spam vs. ham), sentiment analysis (positive vs. negative). 
* In most cases, our real world problem are much more complicated than that.

* Notebook - [Day-17](https://github.com/VeerendraPappala/TEXT-ANALYTICS)

## Day-18 | Convolutional Neural Network with Keras

![keras](https://user-images.githubusercontent.com/45025357/53579507-ad9b8480-3b9f-11e9-9fee-42ba4bb6f056.png)


* CNNs have wide applications in image and video recognition, recommender systems and natural language processing. 
CNNs, like neural networks, are made up of neurons with learnable weights and biases. Each neuron receives several inputs, takes a weighted sum over them, pass it through an activation function and responds with an output.
* Today, I've practised how to implement CNN with Keras

* Notebook - [Day-18](https://github.com/VeerendraPappala/An-Article-A-Day/tree/master/Day-18)

## Day-19 | Ensemble Learing- Random Forest,Ada Boost, Gradient Boosting 

![ensemble-learning-jun29-1](https://user-images.githubusercontent.com/45025357/53652417-d76fac80-3c6e-11e9-995d-804958ae11bc.png)


* Ensemble learning, in general, is a model that makes predictions based on a number of different models. By combining individual models, the ensemble model tends to be more flexibleü§∏‚Äç‚ôÄÔ∏è (less bias) and less data-sensitiveüßò‚Äç‚ôÄÔ∏è (less variance).
* Two most popular ensemble methods are bagging and boosting.
* With a basic understanding of what ensemble learning learnt Random Forest, AdaBoost, and Gradient Boosting, and their implementation in Python Sklearn.

* Notebook - [Day-19](https://github.com/lilly-chen/Bite-sized-Machine-Learning/blob/master/Tree-BasedEsemble/Basic%20Ensemble%20Learning%20-%20Sample%20Code.ipynb)

## Day-20 | Essentials of Deep Learning ‚Äì Sequence to Sequence modelling with Attention (using python)

![sequence modeling](https://user-images.githubusercontent.com/45025357/53682306-769dae00-3d1a-11e9-9b2f-9241929f128f.png)


* We know that to solve sequence modelling problems, Recurrent Neural Networks is our go-to architecture.
* Suppose you have a series of statements:

Joe went to the kitchen. Fred went to the kitchen. Joe picked up the milk.
Joe travelled to the office. Joe left the milk. Joe went to the bathroom.

And you have been asked the below question:

Where was Joe before the office?

* The appropriate answer would be ‚Äúkitchen‚Äù. A quick glance makes this seem like a simple problem. But to understand the complexity ‚Äì there are two dimensions which the system has to understand:

* The underlying working of the English language and the sequence of characters/words which make up the sentence
The sequence of events which revolve around the people mentioned in the statements
This can be considered as a sequence modelling problem, as understanding the sequence is important to make any prediction around it.

* Notebook -[Day-20](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py)]

## Day-21 | Lingvo: A TensorFlow Framework for Sequence Modeling

![linguo](https://user-images.githubusercontent.com/45025357/53693001-68ee3400-3dbf-11e9-8569-6aaca5a4d799.png)


* Lingvo is the international language Esperanto word for ‚Äúlanguage‚Äù. This naming alludes to the roots of the Lingvo framework‚Ää‚Äî‚Ääit was developed as a general deep learning framework using TensorFlow with a focus on sequence models for language-related tasks such as machine translation, speech recognition, and speech synthesis.
* Above picture demontrates an overview of the Lingvo framework, outlining how models are instantiated, trained, and exported for evaluation and serving.
* To jump straight into the code, check out Tensoflow [github](https://github.com/tensorflow/lingvo) page.
* [More](https://arxiv.org/abs/1902.08295) details about Lingvo or some of the advanced features it supports.

## Day-22 | 9 obscure Python libraries for data science

![python libraries](https://user-images.githubusercontent.com/45025357/53747053-717e6180-3ec8-11e9-9ea0-6ff7301d0106.png)


* Python is an amazing language. In fact, it's one of the fastest growing programming languages in the world.
* The entire ecosystem of Python and its libraries makes it an apt choice for users (beginners and advanced) all over the world. One of the reasons for its success and popularity is its set of robust libraries that make it so dynamic and fast.
* This article is about Python libraries for data science tasks other than the commonly used ones like pandas, scikit-learn, and matplotlib.
* Article - [Day-22](https://opensource.com/article/18/11/python-libraries-data-science)

## Day-23 |  TextBlob

![textblob](https://user-images.githubusercontent.com/45025357/53821658-4103f980-3f94-11e9-95a2-3854920f5721.png)

* A Python library for processing textual data, NLP framework, sentiment analysis.
* As an NLP library for Python, TextBlob has been around for a while, after hearing many good things about it such as part-of-speech tagging and sentiment analysi.
* Notebook - [Day-23](https://github.com/susanli2016/NLP-with-Python/blob/master/TextBlob%20Yelp%20Reviews.ipynb)

## Day-24 | Is your Machine Learning Model Biased?

![machine learning](https://user-images.githubusercontent.com/45025357/53898306-d4553180-405d-11e9-9c5c-788ce690427a.jpg)


* Machine learning models are being increasingly used to make decisions that affect people‚Äôs lives. With this power comes a responsibility to ensure that the model predictions are fair and not discriminating.
* This article is about how to measure your model‚Äôs fairness and decide on the best fairness metrics.
* Article - [Day-24](https://towardsdatascience.com/is-your-machine-learning-model-biased-94f9ee176b67)

## Day-25 | A Gentle Introduction to Graph Neural Networks

![graphical neural network](https://user-images.githubusercontent.com/45025357/54041521-f1206f00-41ed-11e9-92e0-a9450eac56d5.png)


* Recently, Graph Neural Network (GNN) has gained increasing popularity in various domains, including social network, knowledge graph, recommender system, and even life science. 
* The power of GNN in modeling the dependencies between nodes in a graph enables the breakthrough in the research area related to graph analysis.
* This article aims to introduce the basics of Graph Neural Network and two more advanced algorithms, DeepWalk and GraphSage.
* Article - [Day-25](https://towardsdatascience.com/a-gentle-introduction-to-graph-neural-network-basics-deepwalk-and-graphsage-db5d540d50b3)

## Day-26 | An Excessively Deep Dive Into Natural Gradient Optimization

* All modern deep learning models are trained using gradient descent. At each step of gradient descent, your parameter values begin at some starting point, and you move them in the direction of greatest loss reduction. 
* You do this by taking the derivative of your loss with respect to your whole vector of parameters, otherwise called the Jacobian. 
* However, this is just the first derivative of your loss, and it doesn‚Äôt tell you anything about curvature, or, how quickly your first derivative is changing. More about article is below.

* Article - [Day-25](https://towardsdatascience.com/its-only-natural-an-excessively-deep-dive-into-natural-gradient-optimization-75d464b89dbb) 




