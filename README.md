# An-Article-A-Day

## Day-1 | Build Book Recommender Systems. 


![book-shelf](https://user-images.githubusercontent.com/45025357/52583901-b168a080-2e56-11e9-8ab2-511ace369f80.jpg)

  **Article Is about Building Recommendation System for Books using Booksdata Set.** 
  
* Virtually everyone has had an online experience where a website makes personalized recommendations. Amazon tells you “Customers Who Bought This Item Also Bought”, Udemy tells you “Students Who Viewed This Course Also Viewed”. Building recommender systems today requires specialized expertise in analytics, machine learning and software engineering, and learning new skills and tools is difficult and time-consuming.
* So, if you want to learn how to build a recommender system from scratch, Read  the complete article from [here](https://towardsdatascience.com/how-did-we-build-book-recommender-systems-in-an-hour-part-2-k-nearest-neighbors-and-matrix-c04b3c2ef55c)  
* Checkout the code from here -  [DAY1](https://github.com/VeerendraPappala/BOOK-RECOMMENDATION-SYSTEM/blob/master/Books_Recommendation.ipynb)


## Day-2 | Summarize Trump’s State of the Union Address

![trump](https://user-images.githubusercontent.com/45025357/52652831-d3c0f380-2f14-11e9-887b-a564feaee770.jpg)

**Article Is about build a Text summarizer so as to summarize Trumps reactions on State of White Houses instead of listening entire speech for  82minutes** 

* Automatic text summarization, is the process of creating a short, concise and coherent version of a longer document
 
* Read the complete [Article](https://towardsdatascience.com/automatically-summarize-trumps-state-of-the-union-address-6757c6af6534)

* Checkout the code from here - [Day2](https://github.com/VeerendraPappala/Analyise-Summarize-Trump-s-remarks-on-state-of-the-Union-Houses/blob/master/Summarize-Trump's-Remarks.ipynb)

## Day-3 | Time Forecast using  Tpot Automated ML in python

![tpot-logo](https://user-images.githubusercontent.com/45025357/52727507-5f9d5300-2fdb-11e9-99b7-d86acc80f44a.jpg)

**Article is about Time Forecast Using TPOT- An automated Machine Learning In Python**

* TPOT is meant to be an assistant that gives you ideas on how to solve a particular machine learning problem by exploring pipeline configurations that you might have never considered, then leaves the fine-tuning to more constrained parameter tuning techniques such as grid search.

* So TPOT helps you find good algorithms. Note that it isn’t designed for automating deep learning — something like AutoKeras might be helpful there.
* AutoML algorithms aren’t as simple as fitting one model on the dataset; they are considering multiple machine learning algorithms (random forests, linear models, SVMs, etc.) in a pipeline with multiple preprocessing steps (missing value imputation, scaling, PCA, feature selection, etc.), the hyperparameters for all of the models and preprocessing steps, as well as multiple ways to ensemble or stack the algorithms within the pipeline.

* There are so many interesting directions to explore with TPOT and autoML. I’d like to compare TPOT with autoSKlearn, MLBox, Auto-Keras, and others. I’d also like to see how it performs with a greater variety of data, other imputation strategies, and other encoding strategies. A comparison with LightGBM, CatBoost , and deep learning algorithms would also be interesting. 

* For More info info Read the complete [Article](https://towardsdatascience.com/time-forecast-with-tpot-b2d87eaba59c) & [Documentation](http://epistasislab.github.io/tpot/using/)

* Checkout the complete Code here - [Day3](https://github.com/VeerendraPappala/Time-Forecast-Using-TPOT/blob/master/TPOT.ipynb)

## Day-4 | PYTORCH AND ResNet for Traffic Sign Classification With PyTorch

![pytorch-logo](https://user-images.githubusercontent.com/45025357/52785901-36360300-307f-11e9-97f4-d01c619a51ab.jpg)

**Article is Restnet used  with pytorch to classify German Traffic Board signs using Fastai in Python** 

* PyTorch was one of the most popular frameworks in 2018.PyTorch is a Python based scientific computing package that is similar to NumPy, but with the added power of GPUs. It is also a deep learning framework that provides maximum flexibility and speed during implementing and building deep neural network architectures.

* The FastAi library is a high level library build on PyTorch, which allows us to build models using only a few lines of code.The FastAI library provides a lot of different datasets which can be loaded in directly, but it also provides functionality for downloading images given a file containing the urls of these images.

* Fastai  allows us to perform transfer learning with less code and time by giving us the ability to set different learning rates for different parts in the network. This allows us to train the earlier layers less than the latter layers.

* Get here  more Info on [pytorch official Guide](https://pytorch.org/tutorials/), 
[Deep Learning with PyTorch](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)
,[ Introduction on pytorch](https://www.analyticsvidhya.com/blog/2019/01/guide-pytorch-neural-networks-case-studies/),  [Resnet for traffic signs Classification using Pytorch](https://towardsdatascience.com/resnet-for-traffic-sign-classification-with-pytorch-5883a97bbaa3)

* Code for - [Day4](https://github.com/VeerendraPappala/PYTORCH/blob/master/Resnet.ipynb)


## Day-5 | Google Facets & Bookeh For Data Visualisation in Python

![data-visualization-tools-in-python-15-638](https://user-images.githubusercontent.com/45025357/52860670-1b828d80-3156-11e9-98f1-b0b950b93ae8.jpg)

**Visualising Machine Learning Datasets with Google’s FACETS - An open source tool from Google to easily learn patterns from large amounts of data And Bokeh**

* Data visualization is important to create meaningful information and decision making power. Data is useless if its not creating any meaning. I did  EDA with FACETS on the universal Dataset and Heart Disease dataset from Kaggle. 
* more info is here 
  * [Article on Facets](https://towardsdatascience.com/visualising-machine-learning-datasets-with-googles-facets-462d923251b3)            
  * [Article on Bokeh](https://towardsdatascience.com/interactive-plotting-with-bokeh-ea40ab10870)
  * [Documentation on Bokeh](https://bokeh.pydata.org/en/latest/docs/user_guide/layout.html)
  * [Facets](https://ai.googleblog.com/2017/07/facets-open-source-visualization-tool.html)
  
* Notebooks of Day-5 - [Facets](https://github.com/VeerendraPappala/Data-Visualization/tree/master/FACETS) & [Bokeh](https://github.com/bokeh/bokeh-notebooks/tree/master/tutorial)



## Day-6 | Develop a NLP Model in Python & Deploy It with Flask


![flask](https://user-images.githubusercontent.com/45025357/52902710-9a50f680-323a-11e9-92ea-49942e0b974b.png)

**In reality, generating predictions is only part of a machine learning project, although it is the most important part . This article is about how to deploying a model in Production using Flask.**

* For more info, chekout these articles
  * [Tutorial to deploy Machine Learning models in Production as APIs](https://www.analyticsvidhya.com/blog/2017/09/machine-learning-models-as-apis-using-flask/)
  * [Deploying a Machine Learning Model as a REST API](https://towardsdatascience.com/deploying-a-machine-learning-model-as-a-rest-api-4a03b865c166)
  * [NLP model Deployement Using Flask](https://towardsdatascience.com/develop-a-nlp-model-in-python-deploy-it-with-flask-step-by-step-744f3bdd7776)
  
* The complete working source code is available at this repository - [Day-6](https://github.com/susanli2016/SMS-Message-Spam-Detector) 


## Day-7 | PyViz: Simplifying the Data Visualisation process in Python.

![pyviz](https://user-images.githubusercontent.com/45025357/52916608-b8d0f380-3307-11e9-8b64-9e6448802ea3.png)

**Article today is about An overview of the PyViz ecosystem to make data visualizations in Python easier to use, learn and more powerful.**

* Now, to choose the best tool for our job from amongst all of the above  is a bit tricky and confusing. PyViz tries to plug this situation. It helps to streamline the process of working with small and large datasets (from a few points to billions) in a web browser, whether doing exploratory analysis, making simple widget-based tools or building full-featured dashboards.
* HoloViews : Declarative objects for instantly visualizable data, building Bokeh plots from convenient high-level specifications.
* The open source libraries which constitute PyViz are:
  * GeoViews 
  * Datashader
  * hvPlot 
  * Param 
  * Holoviews
  * Bokeh
  * Panel

* Get more info by reading these [Article by Parul Pandey](https://towardsdatascience.com/pyviz-simplifying-the-data-visualisation-process-in-python-1b6d2cb728f1), [Pyviz Documentation](http://pyviz.org).
* The dataset used in practise is [Uniqlo (FastRetailing) Stock Price Prediction](https://www.kaggle.com/daiearth22/uniqlo-fastretailing-stock-price-prediction) from Kaggle.

* Here is the Data and Notebook of  [Day-7](https://github.com/VeerendraPappala/Data-Visualization/tree/master/Pyviz)

## DAY-8 |  A Linear Regression with PySpark and MLlib

![pyspark](https://user-images.githubusercontent.com/45025357/52965852-cb146580-33cb-11e9-91ba-af4f54fad2e3.png)

**Apache Spark has become one of the most commonly used and supported open-source tools for machine learning and data science.**
*  From this [Article](https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a),I got to know how to  get started using Apache Spark’s.
* Apache Spark’s.spark.ml Linear Regression is used for predicting Boston housing prices. 
*  data is from the [Kaggle competition](https://www.kaggle.com/c/boston-housing/data)
* For more info on pyspark dive into these links [Apache Spark](https://spark.apache.org), [Apache spark for classification & Regression](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#linear-regression)
* Notebook of  [Day-8](https://github.com/VeerendraPappala/PYSPARK)

## DAY-9 | A Logistic Regression with Pyspark

![merge](https://user-images.githubusercontent.com/45025357/53030592-23fa0180-3491-11e9-9fad-5e1a1daf5bfd.jpg)

* From this [Article](https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a),I got to know how to  get started using Apache Spark’s. 
* As the article tells about linear regression, I built a logistic regression with Apache Spark using [banks marketing Dataset](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) taken from UCI. 
* For more info on [Apache Spark](https://spark.apache.org), [Apache spark for classification & Regression](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#linear-regression).
 
* Notebook of [Day-9](https://github.com/VeerendraPappala/PYSPARK/tree/master/Logistic%20Regression)

## DAY-10 | K-Means with Pyspark

![k-means_convergence](https://user-images.githubusercontent.com/45025357/53107879-05ad0800-355c-11e9-884b-8528fe3aed1f.gif)

*  k-means clustering aims to converge on an optimal set of cluster centers (centroids) and cluster membership based on distance from these centroids via successive iterations, it is intuitive that the more optimal the positioning of these initial centroids, the fewer iterations of the k-means clustering algorithms will be required for convergence. 
* Today I've learned  kmeans implementation using pyspark in python. 
* Article on K-means uisng pyspark is [here](https://towardsdatascience.com/k-means-implementation-in-python-and-spark-856e7eb5fe9b) 
* Notebook of [Day-9](https://github.com/VeerendraPappala/PYSPARK/tree/master/K-MEANS)

## Day-11 | Classification with Pyspark

* Binary classification involves classifying the data into two groups, e.g. whether or not a customer buys a particular product or not (Yes/No), based on independent variables such as gender, age, location etc.
* As the target variable is not continuous, binary classification model predicts the probability of a target variable to be Yes/No. 
* I learned implementing  Binary Classification using Pyspark in Python as of today's work
* [Article](https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a) I learned today. 
* Notebook of [Day-10](https://github.com/VeerendraPappala/PYSPARK)





